{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/chispa/Dropbox/PhD/2019/2/Papers/Paper_1/Published_Code/')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from network import Network\n",
    "from preprocesser import Preprocesser\n",
    "import json\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository contains the code needed to train and evaluate the model presented in [Scalable End-to-end Recurrent Neural Network for Variable star classification\n",
    "](https://academic.oup.com/mnras/article/493/2/2981/5728517)\n",
    "\n",
    "All the datasets used are available at this [link](https://drive.google.com/drive/folders/1Ywjz8RKq8fsqQrK3NBiFUVAs1P17y13I?usp=sharing).\n",
    "Each file contains the light curves compressed and a csv file, wich contains the ID of the object, its class, the number of observations and the relative path ob the light curve of each object.\n",
    "\n",
    "An example for the Gaia DR2 dataset is shown below.\n",
    "\n",
    "|    |                  ID | Class      | Path                          |   N |\n",
    "|----|---------------------|------------|-------------------------------|-----|\n",
    "| 47 | 5953061724418560000 | MIRA_SR    | ./LCs/5953061724418560000.dat |  25 |\n",
    "| 49 | 4336314533849303296 | RRAB       | ./LCs/4336314533849303296.dat |  19 |\n",
    "|  3 | 4041877208419516544 | RRAB       | ./LCs/4041877208419516544.dat |  14 |\n",
    "|  7 | 1745948461173088128 | RRC        | ./LCs/1745948461173088128.dat |  12 |\n",
    "| 22 | 6199756429598744832 | DSCT_SXPHE | ./LCs/6199756429598744832.dat |  20 |\n",
    "\n",
    "To train the model, the data has to be preprocessed first. \n",
    "\n",
    "First, we create the Preprocesser object with the following parameters:\n",
    "1. **max_L**: The maximum number of lightcurves per class.\n",
    "2. **min_L**: The minimum number of light curves per class. If the number is not enough, the class is discarded.\n",
    "3. **min_N**: The minimum number of observations of the light curves.\n",
    "4. **max_N**: The maximum number of observations of the light curves.\n",
    "5. **w**: The size of the sliding window.\n",
    "6. **s**: The step of the sliding window.\n",
    "7. **w_time**: Whether to use the time information of the light curves. \n",
    "8. **lc_parameters**: A dictionary containing the parameters given to pandas.read_csv to read each light curve.\n",
    "9. **num_cores**: Number of threads to use in the preprocessing stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_L = 40000\n",
    "min_L=500\n",
    "min_N=20\n",
    "max_N=2000\n",
    "w=4\n",
    "s=2\n",
    "w_time=True\n",
    "lc_parameters = {'header':0, 'na_filter':False,'sep':',','usecols':['time', 'mag', 'mag_err']}\n",
    "num_cores=4\n",
    "P = Preprocesser(max_L=max_L,\n",
    "                min_L=min_L,\n",
    "                min_N=min_N, \n",
    "                max_N=max_N, \n",
    "                w=w,\n",
    "                s=s,\n",
    "                w_time=w_time, \n",
    "                lc_parameters=lc_parameters,\n",
    "                num_cores=num_cores\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the object is created, we preprocess the data with the function **prepare**. \n",
    "\n",
    "The parameters needed are:\n",
    "1. **file_train**: The csv file containing ID, path, class and number of observations, per object.\n",
    "2. **save_dir**: The path to store the preprocessed files and metadata.\n",
    "3. **train_size**: the proportion to data to be included in the training set. \n",
    "4. **test_size**: the proportion to data to be included in the test set.\n",
    "5. **val_size**: the proportion to data to be included in the validation set.\n",
    "\n",
    "If he user wants to add their own splits, **train_size**, **test_size** and **val_size** must have the same structure as **file_train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114442it [01:08, 1666.97it/s]\n",
      " 30%|██▉       | 6234/21116 [00:00<00:00, 46760.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21116/21116 [00:00<00:00, 37066.38it/s]\n",
      "100%|██████████| 40000/40000 [00:01<00:00, 27412.86it/s]\n",
      "100%|██████████| 6274/6274 [00:00<00:00, 58820.91it/s]\n",
      "100%|██████████| 1308/1308 [00:00<00:00, 24698.59it/s]\n",
      "100%|██████████| 5178/5178 [00:00<00:00, 62557.73it/s]\n",
      "100%|██████████| 40000/40000 [00:01<00:00, 32971.36it/s]\n",
      "100%|██████████| 566/566 [00:00<00:00, 51979.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 1.50 minutes.\n"
     ]
    }
   ],
   "source": [
    "t_ini = time()\n",
    "\n",
    "file_train = './GAIA_dataset.dat'\n",
    "save_dir = './Output/'\n",
    "train_size = 0.7\n",
    "test_size = 0.2\n",
    "val_size = 0.1\n",
    "P.prepare(file_train=file_train, save_dir=save_dir, train_size=train_size, test_size=test_size, val_size=val_size)\n",
    "\n",
    "t_end = time()\n",
    "print('Time elapsed: {:2.2f} minutes.'.format((t_end-t_ini)/60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once preprocessed, the folder **save_dir** will contain the serialized files **Train.tfrecord**, **Test.tfrecord** and **Val.tfrecord**.\n",
    "Additionally, it will store **metadata_preprocess.json** containing the metadata of the preocess and a numpy serialized file containing **lc_parameters**.\n",
    "\n",
    "To preprocess another dataset without validation splits, the Preprocesser object has the function **prepare_inference**.\n",
    "The parameters needed are:\n",
    "1. **file_train**: The csv file containing ID, path, class and number of observations, per object.\n",
    "2. **save_dir**: The path to store the preprocessed files and metadata.\n",
    "3. **metadata_path**: The path of the _metadata_preprocess.json_ file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 7936.54it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 953.31it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 7330.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1207.34it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 4220.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1313.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading\n",
      "Processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_train = './GAIA_dataset_inference.dat'\n",
    "save_path = './Output/Inference.tfrecord'\n",
    "metadata_path = './Output/metadata_preprocess.json'\n",
    "P.prepare_inference(file_train=file_train\n",
    "                    , save_path=save_path\n",
    "                    , metadata_path=metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, we create the object Network without any parameters.\n",
    "\n",
    "The function _train_ receives the dictionary **train_args**, and the serialized paths **tfrecords_trainfor** and **tfrecords_val**, to the training and validation files, respectively.\n",
    "\n",
    "It contains the following keys:\n",
    "1. **epochs**: The number of epochs used to train the model.\n",
    "2. **size_hidden**: Size of the hidden state.\n",
    "3. **rnn_layers**: Number of recurrent layers.\n",
    "4. **lr**: learning rate.\n",
    "5. **fc_layers**: Number of fully connected layers to be applied after the recurrent portion\n",
    "6. **fc_units**: Size of the fully connected layer. By default is the double of the hidden state size.\n",
    "7. **batch_size**: Size of the batch in the training stage. \n",
    "8. **dropout**: Percentaje of dropout used in the fully connected layers. Default 0.\n",
    "9. **val_steps**: Number of training steps before evaluating in the validation set.\n",
    "10. **num_cores**: Number of cores used to deserialize the information and feed the GPU. \n",
    "11. **buffer_size**: Size of the buffer which shuffles the data.\n",
    "12. **max_to_keep**: Maximum number of models to keep.\n",
    "13. **metadata_pre_path**: Path of _metadata_preprocess.json_ file.\n",
    "14. **buffer_size**: Size of the shuffle buffer.\n",
    "15. **num_cores**: Number of threads to use in the input pipeline.\n",
    "16. **save_dir**: Path to save the training data.\n",
    "\n",
    "In the path specified in **save_dir**, the folders _Model_ to store the model checkpoints and _Logs_ which can be visualized in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:347: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:348: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:350: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/chispa/Dropbox/PhD/2019/2/Papers/Paper_1/Published_Code/network.py:111: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/chispa/Dropbox/PhD/2019/2/Papers/Paper_1/Published_Code/network.py:208: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/chispa/Dropbox/PhD/2019/2/Papers/Paper_1/Published_Code/network.py:209: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/chispa/Dropbox/PhD/2019/2/Papers/Paper_1/Published_Code/network.py:134: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "Prediction accuracy on Train set: 32.40%\n",
      "Prediction accuracy on Val set: 31.37%\n",
      "INFO:tensorflow:./Results/Model/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Training ended\n",
      "Time elapsed: 0.31 minutes.\n"
     ]
    }
   ],
   "source": [
    "t_ini = time()\n",
    "data_dir = './Output/'\n",
    "tfrecords_train = [data_dir+'Train.tfrecord']\n",
    "tfrecords_val = [data_dir+'Val.tfrecord']\n",
    "\n",
    "\n",
    "train_args = {\n",
    "'size_hidden' : 25,\n",
    "'rnn_layers' : 2,\n",
    "'fc_units' : 50,\n",
    "'fc_layers' : 1,\n",
    "'buffer_size' : 40000,\n",
    "'epochs' : 5,\n",
    "'num_cores' : 7,\n",
    "'batch_size' : 2500,\n",
    "'dropout' : 0.4,\n",
    "'lr' : 1e-2,\n",
    "'val_steps' : 1,\n",
    "'max_to_keep' : 0,\n",
    "'metadata_pre_path': data_dir+'metadata_preprocess.json',\n",
    "'save_dir' : './Results/'\n",
    "}\n",
    "\n",
    "net = Network()\n",
    "net.train(train_args, tfrecords_train, tfrecords_val)\n",
    "t_end = time()\n",
    "print('Time elapsed: {:2.2f} minutes.'.format((t_end-t_ini)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training ends, we can predict using the predict method. \n",
    "\n",
    "The parameters are:\n",
    "1. **tfrecords**: List of tfrecord files. The results will be concatenated.\n",
    "2. **model_name**: Path to the model files identified with the model number. Example, './Model/model.ckpt-0'.\n",
    "3. **metadata_train_path**: Path to the train metadata, located at './Model/metadata_train.json'.\n",
    "4. **return_h**: Boolean. Wether to return the hidden state after the RNN section.\n",
    "5. **return_p**: Boolean. Wether to return the classification probability.\n",
    "\n",
    "It returns a dictionary with keys _ids_, _labels_, _pred_label_. If selected,  _pred_probs_ and _last_h_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Results/Model/model.ckpt-0\n"
     ]
    }
   ],
   "source": [
    "tfrecords_test = [data_dir+'Test.tfrecord', data_dir+'Inference.tfrecord']\n",
    "model_name = './Results/Model/model.ckpt-0'\n",
    "metadata_train_path = './Results/Model/metadata_train.json'\n",
    "\n",
    "net = Network()\n",
    "predictions = net.predict(tfrecords_test, model_name, metadata_train_path, return_h=False, return_p=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5385985372548272128</td>\n",
       "      <td>RRAB</td>\n",
       "      <td>RRAB</td>\n",
       "      <td>0.191029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6248180369093869440</td>\n",
       "      <td>RRC</td>\n",
       "      <td>RRAB</td>\n",
       "      <td>0.191314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5951956131140637440</td>\n",
       "      <td>RRC</td>\n",
       "      <td>RRAB</td>\n",
       "      <td>0.224031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5968726840210212352</td>\n",
       "      <td>MIRA_SR</td>\n",
       "      <td>RRAB</td>\n",
       "      <td>0.207551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5927671080359624064</td>\n",
       "      <td>MIRA_SR</td>\n",
       "      <td>RRAB</td>\n",
       "      <td>0.198111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ids   labels pred_label  pred_probs\n",
       "0  5385985372548272128     RRAB       RRAB    0.191029\n",
       "1  6248180369093869440      RRC       RRAB    0.191314\n",
       "2  5951956131140637440      RRC       RRAB    0.224031\n",
       "3  5968726840210212352  MIRA_SR       RRAB    0.207551\n",
       "4  5927671080359624064  MIRA_SR       RRAB    0.198111"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(predictions)\n",
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
